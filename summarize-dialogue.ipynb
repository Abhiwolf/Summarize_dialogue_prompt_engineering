{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.15.0  --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoTokenizer\nfrom transformers import GenerationConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 - **Summarize Dialogue without Prompt Engineering**\nIn this use case, you will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. The list of available models in the Hugging Face transformers package can be found here.\n\nLet's upload some simple dialogues from the DialogSum Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"knkarthick/dialogsum\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_indices = [40, 200]\ndash_line = \"-\".join(\"\" for x in range(100))\nfor i, index in enumerate(example_indices):\n    print(index)\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print('INPUT DIALOGUE:')\n    print(dataset['test'][index]['dialogue'])\n    print(dash_line)\n    print('BASELINE HUMAN SUMMARY:')\n    print(dataset['test'][index]['summary'])\n    print(dash_line)\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the FLAN-T5 model, creating an instance of the AutoModelForSeq2SeqLM class with the .from_pretrained() method.","metadata":{}},{"cell_type":"code","source":"model_name='google/flan-t5-base'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To perform encoding and decoding, you need to work with text in a tokenized form. Tokenization is the process of splitting texts into smaller units that can be processed by the LLM models.\n\nDownload the tokenizer for the FLAN-T5 model using AutoTokenizer.from_pretrained() method. Parameter use_fast switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test the tokenizer encoding and decoding a simple sentence:","metadata":{}},{"cell_type":"code","source":"sentence = \"What time is it, Tom?\"\nsentence_encoded = tokenizer(sentence, return_tensors='pt')\n\nsentence_decoded = tokenizer.decode(\n        sentence_encoded[\"input_ids\"][0], \n        skip_special_tokens=True\n    )\n\nprint('ENCODED SENTENCE:')\nprint(sentence_encoded[\"input_ids\"][0])\nprint('\\nDECODED SENTENCE:')\nprint(sentence_decoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. Prompt engineering is an act of a human changing the prompt (input) to improve the response for a given task.","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n    \n    inputs = tokenizer(dialogue, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n    \n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{dialogue}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)\n    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that the guesses of the model make some sense, but it doesn't seem to be sure what task it is supposed to accomplish. Seems it just makes up the next sentence in the dialogue. Prompt engineering can help here.\n","metadata":{}},{"cell_type":"markdown","source":"**3 - Summarize Dialogue with an Instruction Prompt**\nPrompt engineering is an important concept in using foundation models for text generation. You can check out this blog from Amazon Science for a quick introduction to prompt engineering.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"**3.1 - Zero Shot Inference with an Instruction Prompt**\nIn order to instruct the model to perform a task - summarize a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called zero shot inference. You can check out this blog from AWS for a quick description of what zero shot learning is and why it is an important concept to the LLM model.\n\nWrap the dialogue in a descriptive instruction and see how the generated text will change:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n    \"\"\"\n\n    # Input constructed prompt instead of the dialogue.\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n    \n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n    print(dash_line)    \n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is much better! But the model still does not pick up on the nuance of the conversations though.","metadata":{}},{"cell_type":"markdown","source":"**3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5**","metadata":{}},{"cell_type":"markdown","source":"Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks here. In the following code, you will use one of the pre-built FLAN-T5 prompts:","metadata":{}},{"cell_type":"code","source":"for i, index in enumerate(example_indices):\n    dialogue = dataset['test'][index]['dialogue']\n    summary = dataset['test'][index]['summary']\n        \n    prompt = f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors='pt')\n    output = tokenizer.decode(\n        model.generate(\n            inputs[\"input_ids\"], \n            max_new_tokens=50,\n        )[0], \n        skip_special_tokens=True\n    )\n\n    print(dash_line)\n    print('Example ', i + 1)\n    print(dash_line)\n    print(f'INPUT PROMPT:\\n{prompt}')\n    print(dash_line)\n    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n    print(dash_line)\n    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what you will try to solve with the few shot inferencing.","metadata":{}},{"cell_type":"markdown","source":"**4 - Summarize Dialogue with One Shot and Few Shot Inference**","metadata":{}},{"cell_type":"markdown","source":"One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task. You can read more about it in this blog from HuggingFace.","metadata":{}},{"cell_type":"markdown","source":"**4.1 - One Shot Inference**","metadata":{}},{"cell_type":"markdown","source":"Let's build a function that takes a list of example_indices_full, generates a prompt with full examples, then at the end appends the prompt which you want the model to complete (example_index_to_summarize). You will use the same FLAN-T5 prompt template from section 3.2.\n\n","metadata":{}},{"cell_type":"code","source":"def make_prompt(example_indices_full, example_index_to_summarize):\n    prompt = ''\n    for index in example_indices_full:\n        dialogue = dataset['test'][index]['dialogue']\n        summary = dataset['test'][index]['summary']\n        \n        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n        prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n{summary}\n\n\n\"\"\"\n    \n    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n    \n    prompt += f\"\"\"\nDialogue:\n\n{dialogue}\n\nWhat was going on?\n\"\"\"\n        \n    return prompt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_indices_full = [40]\nexample_index_to_summarize = 200\n\none_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(one_shot_prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now pass this prompt to perform the one shot inference:","metadata":{}},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\n\ninputs = tokenizer(one_shot_prompt, return_tensors='pt')\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ONE SHOT:\\n{output}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.2 - Few Shot Inference**\n\nLet's explore few shot inference by adding two more full dialogue-summary pairs to your prompt.","metadata":{}},{"cell_type":"code","source":"example_indices_full = [40, 80, 120]\nexample_index_to_summarize = 200\n\nfew_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n\nprint(few_shot_prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now pass this prompt to perform a few shot inference:","metadata":{}},{"cell_type":"code","source":"summary = dataset['test'][example_index_to_summarize]['summary']\n\ninputs = tokenizer(few_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either. Also, you need to make sure that you do not exceed the model's input-context length which, in our case, if 512 tokens. Anything above the context length will be ignored.\n\nHowever, you can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall.","metadata":{}},{"cell_type":"markdown","source":"**5 - Generative Configuration Parameters for Inference**","metadata":{}},{"cell_type":"markdown","source":"You can change the configuration parameters of the generate() method to see a different output from the LLM. So far the only parameter that you have been setting was max_new_tokens=50, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the Hugging Face Generation documentation.\n\nA convenient way of organizing the configuration parameters is to use GenerationConfig class.","metadata":{}},{"cell_type":"code","source":"generation_config = GenerationConfig(max_new_tokens=50)\n# generation_config = GenerationConfig(max_new_tokens=10)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n\ninputs = tokenizer(few_shot_prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config=generation_config,\n    )[0], \n    skip_special_tokens=True\n)\n\nprint(dash_line)\nprint(f'MODEL GENERATION - FEW SHOT:\\n{output}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}